{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import Exception\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import mysql.connector\n",
    "import re\n",
    "from datetime import date\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Detik Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fungsi menyimpan data ke database\n",
    "def simpanDatabase(self,data):\n",
    "    mycursor = self.mydb.cursor()\n",
    "    sql = \"INSERT INTO tb_berita (media,judul_berita,label,penulis,release_date,url,content) VALUES (%s,%s,%s,%s,%s,%s,%s)\"\n",
    "    mycursor.execute(sql, data)\n",
    "    self.mydb.commit()\n",
    "    return\n",
    "\n",
    "#fungsi buat validasi data duplicate di database\n",
    "def validasiDataDatabase(self,dataBerita):\n",
    "    mycursor = self.mydb.cursor()\n",
    "    sql = \"SELECT * FROM tb_berita WHERE url = %s\"\n",
    "    dataUrl = dataBerita\n",
    "    url = (dataBerita,)\n",
    "    mycursor.execute(sql, url)\n",
    "    myresult = mycursor.fetchall()\n",
    "    return myresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkSearch = 'https://www.detik.com/search/searchall?query=vaksin&page=2'\n",
    "linNoSearch = 'https://news.detik.com/indeks/1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detik:  \n",
    "    #deklarasi variable global\n",
    "    url = \"\"\n",
    "    mydb = \"\"\n",
    "    keyword = \"\"\n",
    "    urlDetailData = \"\"\n",
    "    today = datetime.now()\n",
    "    \n",
    "    def __init__(self, urlBerita, pencarian):\n",
    "        self.url = urlBerita\n",
    "        self.keyword = pencarian\n",
    "        self.mydb = mysql.connector.connect(host=\"localhost\",user=\"root\", password=\"\",database=\"db-berita-mca\")  \n",
    "    \n",
    "    #fungsi menyimpan data ke database\n",
    "    def simpanDatabase(self,data):\n",
    "        mycursor = self.mydb.cursor()\n",
    "        sql = \"INSERT INTO tb_berita (media,judul_berita,label,penulis,release_date,url,content) VALUES (%s,%s,%s,%s,%s,%s,%s)\"\n",
    "        mycursor.execute(sql, data)\n",
    "        self.mydb.commit()\n",
    "        return\n",
    "    \n",
    "    #fungsi buat validasi data duplicate di database\n",
    "    def validasiDataDatabase(self,dataBerita):\n",
    "        mycursor = self.mydb.cursor()\n",
    "        sql = \"SELECT * FROM tb_berita WHERE url = %s\"\n",
    "        dataUrl = dataBerita\n",
    "        url = (dataBerita,)\n",
    "        mycursor.execute(sql, url)\n",
    "        myresult = mycursor.fetchall()\n",
    "        return myresult\n",
    "\n",
    "    #fungsi buat ngambil jumlah page pada pencarian berita tersebut\n",
    "    def getPage(self):\n",
    "        if(self.keyword == \"\"):\n",
    "            try:\n",
    "                dateNow = today.strftime(\"%m/%d/%Y\")\n",
    "                urlGet = (\"https://news.detik.com/indeks/1?date=\"+dateNow)\n",
    "                soup = BeautifulSoup(get(urlGet).text, 'lxml')\n",
    "                getAllPage= soup.find('div','pagination text-center mgt-16 mgb-16')\n",
    "                page_link = getAllPage.select('a')\n",
    "                for page in page_link :\n",
    "                    if(page.text != \"Next\"):\n",
    "                        lastPage = page.text\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            try:\n",
    "                dateNow = today.strftime(\"%m/%d/%Y\")\n",
    "                urlGet = (self.url+\"/search/searchall?query=\"+self.keyword+\"&fromdatex=\"+dateNow+\"&todatex=\"+dateNow)\n",
    "                soup = BeautifulSoup(get(urlGet).text, 'lxml')\n",
    "                getAllPage= soup.find('div','paging text_center')\n",
    "                page_link = getAllPage.select('a')\n",
    "                for page in page_link:\n",
    "                    if(page.text != \"\"):\n",
    "                        lastPage = page.text\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        return lastPage\n",
    "    \n",
    "    #ngambil semua url-detail pada scraping data\n",
    "    def getDetailUrl(self, page):\n",
    "        self.urlDetailData = []\n",
    "        for page in range(page): \n",
    "            if(self.keyword == \"\"):\n",
    "                dateNow = today.strftime(\"%m/%d/%Y\")\n",
    "                urlGet = (\"https://news.detik.com/indeks/\"+format(page+1)+\"?date=\"+dateNow)\n",
    "            else:\n",
    "                dateNow = today.strftime(\"%d/%m/%Y\")\n",
    "                urlGet = (self.url+\"/search/searchall?query=\"+self.keyword+\"&fromdatex=\"+dateNow+\"&todatex=\"+dateNow+\"&page={}\".format(page+1))\n",
    "            indeksPage = get(urlGet)\n",
    "            soup = BeautifulSoup(indeksPage.text, 'html.parser')\n",
    "            contents = soup.find_all('article')\n",
    "            for content in contents:\n",
    "                try:\n",
    "                    news_url = content.find('a', href=True).get('href')\n",
    "                    self.urlDetailData.append(news_url)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        return self.urlDetailData\n",
    "    \n",
    "    def scrapingDataDetik(self):\n",
    "        #disini aku deklarasiin manual pagenya kalau mau semua page berita yang diambil mati sudah ni engine wkwk\n",
    "        self.getDetailUrl(1)\n",
    "        for dataBerita in self.urlDetailData:\n",
    "            myresult = self.validasiDataDatabase(dataBerita)\n",
    "            if(len(myresult)==0):\n",
    "                try:\n",
    "                    web_data = get(dataBerita)\n",
    "                    soup = BeautifulSoup(web_data.text, 'html.parser')\n",
    "                    header = soup.find('article','detail')\n",
    "                    title = header.find('h1','detail__title').text\n",
    "                    title = re.sub(r'\\s+', ' ',title)\n",
    "                    title = title.lstrip()\n",
    "                    author_class = header.find('div','detail__author').text.split(' ')\n",
    "                    authorOne = author_class[0]\n",
    "                    media = \"Detik.com\"\n",
    "                    label = soup.find('div','page__breadcrumb')\n",
    "                    label = label.find('a',href=True).text\n",
    "                    author = (authorOne+\" & \")+\"\".join(author_class[:2])\n",
    "                    release_date = header.find('div', class_='detail__date').text\n",
    "                    detail_text_class = soup.find('div', class_='detail__body-text')\n",
    "                    texts = detail_text_class.find_all('p')\n",
    "                    news_content = ' '.join([text.text for text in texts])\n",
    "                    data = (media,title,label,author,release_date,dataBerita,news_content)\n",
    "                    self.simpanDatabase(data)\n",
    "                    print(title)\n",
    "                except Exception as e:\n",
    "                    continue    \n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vaksinasi di Tasikmalaya Dikebut Antisipasi Masuknya Omicron \n",
      "Ganjil Genap Jakarta Diperpanjang, Catat Rute dan Jadwalnya \n",
      "Mendagri Minta Pemda Kebut Vaksinasi Corona Pakai APBD hingga CSR \n",
      "Para Badut Temani Anak-anak Usia 6-11 Tahun Vaksin di Kota Pasuruan \n",
      "Antisipasi Omicron, Pemkot Bandung Minta Warga Tak Panik-Perketat Prokes \n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.detik.com'\n",
    "keyword = 'vaksin'\n",
    "\n",
    "detik = Detik(url,keyword)\n",
    "detik.scrapingDataDetik()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
